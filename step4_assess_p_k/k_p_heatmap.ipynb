{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total rows read: 5000\n",
      "k=1, p=1: val_acc=0.843, test_acc=0.860\n",
      "k=1, p=2: val_acc=0.843, test_acc=0.854\n"
     ]
    }
   ],
   "source": [
    "import sns\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "\n",
    "# Initialize the data\n",
    "def initData(filename, training_ratio, validation_ratio, test_ratio, attributes):\n",
    "    with open(filename, 'r', newline='') as file:\n",
    "        file_csv = csv.reader(file)\n",
    "        full_header = next(file_csv)  # Read the full header row\n",
    "\n",
    "        # Identify indices for the chosen attributes\n",
    "        chosen_indices = [full_header.index(attr) for attr in attributes]\n",
    "        labels = []\n",
    "        data = []\n",
    "\n",
    "        for row in file_csv:\n",
    "            if len(row) < len(full_header):\n",
    "                continue\n",
    "            \n",
    "            # Extract chosen attributes and convert to float\n",
    "            features = [float(row[i]) for i in chosen_indices]\n",
    "            \n",
    "            # Assume the last column is the label\n",
    "            label = row[-1]\n",
    "            data.append(features + [label])\n",
    "            \n",
    "            if label not in labels:\n",
    "                labels.append(label)\n",
    "\n",
    "        len_data = len(data)\n",
    "        print(f\"Total rows read: {len_data}\")\n",
    "\n",
    "        # Split into training, validation, and test sets\n",
    "        train_idx = int(training_ratio * len_data)\n",
    "        val_idx = int((training_ratio + validation_ratio) * len_data)\n",
    "        \n",
    "        training_data = data[:train_idx]\n",
    "        validation_data = data[train_idx:val_idx]\n",
    "        test_data = data[val_idx:]\n",
    "\n",
    "        if len(training_data) == 0 or len(validation_data) == 0:\n",
    "            print(\"[ERROR] Training or validation set is empty. Check your data split.\")\n",
    "            exit()\n",
    "    \n",
    "    chosen_header = [full_header[i] for i in chosen_indices]\n",
    "    return chosen_header, training_data, validation_data, test_data, labels\n",
    "\n",
    "\n",
    "# %%\n",
    "def minkowski_distance(point_1, point_2, p):\n",
    "    return sum(abs(a - b)**p for a, b in zip(point_1, point_2))**(1/p)\n",
    "\n",
    "\n",
    "# %%\n",
    "def knn(training_data, new_point, k, p):\n",
    "    distances = [\n",
    "        (minkowski_distance(new_point, row[:-1], p), row[-1])\n",
    "        for row in training_data\n",
    "    ]\n",
    "    distances.sort(key=lambda x: x[0])\n",
    "    \n",
    "    # Perform voting among the k nearest neighbors\n",
    "    votes = {}\n",
    "    for i in range(k):\n",
    "        label = distances[i][1]\n",
    "        votes[label] = votes.get(label, 0) + 1\n",
    "    \n",
    "    return max(votes, key=votes.get)\n",
    "\n",
    "# %%\n",
    "def compute_accuracy(training_data, data, k, p):\n",
    "\n",
    "    if not data:\n",
    "        return 0\n",
    "    correct = sum(\n",
    "        knn(training_data, point[:-1], k, p) == point[-1]\n",
    "        for point in data\n",
    "    )\n",
    "    return correct / len(data)\n",
    "\n",
    "# %%\n",
    "dataset_file = 'final_pollution_dataset.csv'\n",
    "attributes = ['SO2', 'CO', 'Proximity_to_Industrial_Areas']  # Two selected attributes\n",
    "\n",
    "# Split ratios\n",
    "training_ratio = 0.7\n",
    "validation_ratio = 0.2\n",
    "test_ratio = 0.1\n",
    "\n",
    "# Load data\n",
    "chosen_header, training_data, validation_data, test_data, labels = initData(\n",
    "    filename=dataset_file,\n",
    "    training_ratio=training_ratio,\n",
    "    validation_ratio=validation_ratio,\n",
    "    test_ratio=test_ratio,\n",
    "    attributes=attributes\n",
    ")\n",
    "\n",
    "# Initialize 2D arrays with zeros\n",
    "k_range = range(1, 10)\n",
    "p_range = range(1, 6)\n",
    "val_accuracies = [[0 for _ in p_range] for _ in k_range]\n",
    "test_accuracies = [[0 for _ in p_range] for _ in k_range]\n",
    "\n",
    "# Compute accuracies\n",
    "for i, k in enumerate(k_range):\n",
    "    for j, p in enumerate(p_range):\n",
    "        acc_val = compute_accuracy(training_data, validation_data, k, p)\n",
    "        acc_test = compute_accuracy(training_data, test_data, k, p)\n",
    "        print(f\"k={k}, p={p}: val_acc={acc_val:.3f}, test_acc={acc_test:.3f}\")\n",
    "        val_accuracies[i][j] = acc_val\n",
    "        test_accuracies[i][j] = acc_test\n",
    "\n",
    "# Scale the data\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def scale_data(training_data, validation_data, test_data):\n",
    "    \"\"\"\n",
    "    Scale features using StandardScaler while preserving labels\n",
    "    \"\"\"\n",
    "    # Extract features and labels\n",
    "    train_features = np.array([row[:-1] for row in training_data])\n",
    "    val_features = np.array([row[:-1] for row in validation_data])\n",
    "    test_features = np.array([row[:-1] for row in test_data])\n",
    "    \n",
    "    # Get labels\n",
    "    train_labels = [row[-1] for row in training_data]\n",
    "    val_labels = [row[-1] for row in validation_data]\n",
    "    test_labels = [row[-1] for row in test_data]\n",
    "    \n",
    "    # Scale features\n",
    "    scaler = StandardScaler()\n",
    "    train_scaled = scaler.fit_transform(train_features)\n",
    "    val_scaled = scaler.transform(val_features)\n",
    "    test_scaled = scaler.transform(test_features)\n",
    "    \n",
    "    # Combine scaled features with labels\n",
    "    training_scaled = [list(features) + [label] for features, label in zip(train_scaled, train_labels)]\n",
    "    validation_scaled = [list(features) + [label] for features, label in zip(val_scaled, val_labels)]\n",
    "    test_scaled = [list(features) + [label] for features, label in zip(test_scaled, test_labels)]\n",
    "    \n",
    "    return training_scaled, validation_scaled, test_scaled\n",
    "\n",
    "# Add this after data loading in main():\n",
    "training_data, validation_data, test_data = scale_data(training_data, validation_data, test_data)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot heatmaps\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Validation accuracy heatmap\n",
    "sns.heatmap(val_accuracies, annot=True, fmt='.3f', \n",
    "            xticklabels=p_range, yticklabels=k_range,\n",
    "            ax=ax1, cmap='viridis')\n",
    "ax1.set_title('Validation Accuracy')\n",
    "ax1.set_xlabel('p value')\n",
    "ax1.set_ylabel('k value')\n",
    "\n",
    "# Test accuracy heatmap\n",
    "sns.heatmap(test_accuracies, annot=True, fmt='.3f',\n",
    "            xticklabels=p_range, yticklabels=k_range,\n",
    "            ax=ax2, cmap='viridis')\n",
    "ax2.set_title('Test Accuracy')\n",
    "ax2.set_xlabel('p value')\n",
    "ax2.set_ylabel('k value')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('k_p_accuracy_heatmap.png')\n",
    "plt.show()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_ds_envi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
