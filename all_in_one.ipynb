{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# kNN Classification Guide\n",
    "\n",
    "## How to Use the Code\n",
    "\n",
    "### Step 1: Load Necessary Functions\n",
    "Ensure the following functions are loaded in your notebook:\n",
    "- `initData`: Initialize and split the dataset\n",
    "- `minkowski_distance`: Calculate distance between points\n",
    "- `knn`: Perform k-Nearest Neighbors classification\n",
    "- `compute_accuracy`: Calculate model accuracy\n",
    "- `plot_decision_map`: Visualize decision boundaries\n",
    "\n",
    "### Step 2: Initialize the Data\n",
    "1. Set the file path to your dataset:\n",
    "   ```python\n",
    "   dataset_file = 'path/to/updated_pollution_dataset.csv'\n",
    "2. Define the attributes you want to use:\n",
    "    attributes = ['Temperature', 'SO2']  # Choose any two attributes\n",
    "3. Set the parameters for kNN:\n",
    "    k = 6  # Number of neighbors\n",
    "    p = 4  # Minkowski distance parameter\n",
    "    grid_step = 0.1  # step for visualisation\n",
    "4. Define the split ratios:\n",
    "    training_ratio = 0.7\n",
    "    validation_ratio = 0.2\n",
    "    test_ratio = 0.1  # Ennsure the sum of ratio is 1\n",
    "5. Load and split the data:\n",
    "    chosen_header, training_data, validation_data, test_data, labels = initData(\n",
    "        filename=dataset_file,\n",
    "        training_ratio=training_ratio,\n",
    "        validation_ratio=validation_ratio,\n",
    "        test_ratio=test_ratio,\n",
    "        attributes=attributes\n",
    "    )\n",
    "### Step 3: Use Necessary Features\n",
    "1. Classify Sample Data: \n",
    "    Select a random point from the test data:\n",
    "    Calculate distances to all points in the training set and find the first k neighbors:\n",
    "    Predict the label based on the neighbors:\n",
    "    Calculate the vote of each label among the k neighbors:\n",
    "2. Plot the Decision Map:\n",
    "    Visualize the decision boundaries\n",
    "    plot_decision_map(training_data, attributes, k, p, grid_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Initialize the data\n",
    "def initData(filename, training_ratio, validation_ratio, test_ratio, attributes):\n",
    "    with open(filename, 'r', newline='') as file:\n",
    "        file_csv = csv.reader(file)\n",
    "        full_header = next(file_csv)  # Read the full header row\n",
    "\n",
    "        # Identify indices for the chosen attributes\n",
    "        chosen_indices = [full_header.index(attr) for attr in attributes]\n",
    "        labels = []\n",
    "        data = []\n",
    "\n",
    "        for row in file_csv:\n",
    "            if len(row) < len(full_header):\n",
    "                continue\n",
    "            \n",
    "            # Extract chosen attributes and convert to float\n",
    "            features = [float(row[i]) for i in chosen_indices]\n",
    "            \n",
    "            # Assume the last column is the label\n",
    "            label = row[-1]\n",
    "            data.append(features + [label])\n",
    "            \n",
    "            if label not in labels:\n",
    "                labels.append(label)\n",
    "\n",
    "        len_data = len(data)\n",
    "        print(f\"Total rows read: {len_data}\")\n",
    "\n",
    "        # Split into training, validation, and test sets\n",
    "        train_idx = int(training_ratio * len_data)\n",
    "        val_idx = int((training_ratio + validation_ratio) * len_data)\n",
    "        \n",
    "        training_data = data[:train_idx]\n",
    "        validation_data = data[train_idx:val_idx]\n",
    "        test_data = data[val_idx:]\n",
    "\n",
    "        if len(training_data) == 0 or len(validation_data) == 0:\n",
    "            print(\"[ERROR] Training or validation set is empty. Check your data split.\")\n",
    "            exit()\n",
    "    \n",
    "    chosen_header = [full_header[i] for i in chosen_indices]\n",
    "    return chosen_header, training_data, validation_data, test_data, labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def minkowski_distance(point_1, point_2, p):\n",
    "    return sum(abs(a - b)**p for a, b in zip(point_1, point_2))**(1/p)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def knn(training_data, new_point, k, p):\n",
    "    distances = [\n",
    "        (minkowski_distance(new_point, row[:-1], p), row[-1])\n",
    "        for row in training_data\n",
    "    ]\n",
    "    distances.sort(key=lambda x: x[0])\n",
    "    \n",
    "    # Perform voting among the k nearest neighbors\n",
    "    votes = {}\n",
    "    for i in range(k):\n",
    "        label = distances[i][1]\n",
    "        votes[label] = votes.get(label, 0) + 1\n",
    "    \n",
    "    return max(votes, key=votes.get)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy(training_data, data, k, p):\n",
    "\n",
    "    if not data:\n",
    "        return 0\n",
    "    correct = sum(\n",
    "        knn(training_data, point[:-1], k, p) == point[-1]\n",
    "        for point in data\n",
    "    )\n",
    "    return correct / len(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_decision_map(training_data, attributes, k, p, grid_step):\n",
    "    print(\"[DEBUG] Generating decision map... This can take time if the range is large.\")\n",
    "    \n",
    "    # Extract x, y, and labels from the training data\n",
    "    x_vals = [row[0] for row in training_data]\n",
    "    y_vals = [row[1] for row in training_data]\n",
    "    data_labels = [row[-1] for row in training_data]\n",
    "    \n",
    "    # Define the plot range\n",
    "    x_min, x_max = min(x_vals) - 1, max(x_vals) + 1\n",
    "    y_min, y_max = min(y_vals) - 1, max(y_vals) + 1\n",
    "    \n",
    "    # Generate a grid of points for the decision map\n",
    "    xx, yy = np.meshgrid(\n",
    "        np.arange(x_min, x_max, grid_step),\n",
    "        np.arange(y_min, y_max, grid_step)\n",
    "    )\n",
    "    \n",
    "    # Classify each grid point\n",
    "    Z = np.array([\n",
    "        knn(training_data, [mx, my], k, p)\n",
    "        for mx, my in zip(xx.ravel(), yy.ravel())\n",
    "    ]).reshape(xx.shape)\n",
    "    \n",
    "    # Map labels to integers for coloring\n",
    "    unique_labels = sorted(set(data_labels))\n",
    "    colors = plt.cm.viridis(np.linspace(0, 1, len(unique_labels)))\n",
    "    color_map = ListedColormap(colors)\n",
    "    label_to_idx = {lab: idx for idx, lab in enumerate(unique_labels)}\n",
    "    Z_int = np.vectorize(label_to_idx.get)(Z)\n",
    "    \n",
    "    # Plot decision regions\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    contour = ax.pcolormesh(\n",
    "        xx, yy, Z_int,\n",
    "        cmap=color_map, alpha=0.2, shading='auto'\n",
    "    )\n",
    "    \n",
    "    # Plot training points\n",
    "    for label in unique_labels:\n",
    "        mask = [row[-1] == label for row in training_data]\n",
    "        x = [row[0] for row, m in zip(training_data, mask) if m][:100]\n",
    "        y = [row[1] for row, m in zip(training_data, mask) if m][:100]\n",
    "        ax.scatter(\n",
    "            x, y,\n",
    "            c=[color_map(label_to_idx[label])],\n",
    "            label=label,\n",
    "            edgecolor='black', linewidth=0.1,\n",
    "            s=10\n",
    "        )\n",
    "    \n",
    "    # Labels, title, and legend\n",
    "    ax.set_xlabel(attributes[0], fontsize=12)\n",
    "    ax.set_ylabel(attributes[1], fontsize=12)\n",
    "    ax.set_title(f\"Decision Boundaries: {attributes[0]} vs {attributes[1]}\\nk={k}, p={p}, gridstep={grid_step}\", fontsize=14, pad=15)\n",
    "    ax.legend(title=\"Classes\", bbox_to_anchor=(1.05, 1), loc='upper left', fontsize=10)\n",
    "    ax.grid(True, linestyle='--', alpha=0.5)\n",
    "    \n",
    "    # Save and display the figure\n",
    "    plt.tight_layout()\n",
    "    out_filename = f\"dec_map_{attributes[0]}_{attributes[1]}_k{k}_p{p}_gridstep{grid_step}.png\"\n",
    "    plt.savefig(out_filename, dpi=300, bbox_inches='tight')\n",
    "    print(f\"[DEBUG] Decision map saved as '{out_filename}'\")\n",
    "    \n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '_pollution_dataset.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[41], line 13\u001b[0m\n\u001b[1;32m     10\u001b[0m test_ratio \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.5\u001b[39m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# Load data\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m chosen_header, training_data, validation_data, test_data, labels \u001b[38;5;241m=\u001b[39m \u001b[43minitData\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfilename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdataset_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtraining_ratio\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtraining_ratio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_ratio\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_ratio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtest_ratio\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest_ratio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattributes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattributes\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[36], line 3\u001b[0m, in \u001b[0;36minitData\u001b[0;34m(filename, training_ratio, validation_ratio, test_ratio, attributes)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minitData\u001b[39m(filename, training_ratio, validation_ratio, test_ratio, attributes):\n\u001b[0;32m----> 3\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[1;32m      4\u001b[0m         file_csv \u001b[38;5;241m=\u001b[39m csv\u001b[38;5;241m.\u001b[39mreader(file)\n\u001b[1;32m      5\u001b[0m         full_header \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(file_csv)  \u001b[38;5;66;03m# Read the full header row\u001b[39;00m\n",
      "File \u001b[0;32m~/Programming/python3/ml_ds_general/ml_ds_envi/lib/python3.12/site-packages/IPython/core/interactiveshell.py:324\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[1;32m    318\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    319\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    320\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    321\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    322\u001b[0m     )\n\u001b[0;32m--> 324\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '_pollution_dataset.csv'"
     ]
    }
   ],
   "source": [
    "dataset_file = 'final_pollution_dataset.csv'\n",
    "attributes = ['SO2', 'CO', 'Proximity_to_Industrial_Areas']  # Two selected attributes\n",
    "k = 6\n",
    "p = 4\n",
    "grid_step = 0.1  # Grid step size for faster computation\n",
    "\n",
    "# Split ratios\n",
    "training_ratio = 0.7\n",
    "validation_ratio = 0.2\n",
    "test_ratio = 0.5\n",
    "\n",
    "# Load data\n",
    "chosen_header, training_data, validation_data, test_data, labels = initData(\n",
    "    filename=dataset_file,\n",
    "    training_ratio=training_ratio,\n",
    "    validation_ratio=validation_ratio,\n",
    "    test_ratio=test_ratio,\n",
    "    attributes=attributes\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# Choose a sample point to demonstrate the classification process\n",
    "# input: a random point in the validation set, the corresponding attribute values\n",
    "# output: first k neighbors and the predicted label\n",
    "#         the vote of each label among the k neighbors\n",
    "#         Hence the predicted label\n",
    "\n",
    "# Select a random point from the validation set\n",
    "#sample_point = random.choice(test_data)\n",
    "sample_point = test_data[300]\n",
    "print(f\"Sample point: {sample_point}\")\n",
    "\n",
    "# Calculate distances to all points in the training set\n",
    "neighbors = [\n",
    "    (minkowski_distance(sample_point[:-1], row[:-1], p), row[-1])\n",
    "    for row in training_data\n",
    "]\n",
    "\n",
    "# Sort and select the first k neighbors\n",
    "neighbors.sort(key=lambda x: x[0])\n",
    "first_k_neighbors = neighbors[:k]\n",
    "print(f\"First {k} neighbors: {first_k_neighbors}\")\n",
    "\n",
    "# Predict the label based on the neighbors\n",
    "predicted_label = knn(training_data, sample_point[:-1], k, p)\n",
    "print(f\"Predicted label: {predicted_label}\")\n",
    "\n",
    "# Calculate the vote of each label among the k neighbors\n",
    "votes = {}\n",
    "for distance, label in first_k_neighbors:\n",
    "    votes[label] = votes.get(label, 0) + 1\n",
    "\n",
    "print(f\"Votes: {votes}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy before scale\n",
    "# Compute accuracy\n",
    "acc_val = compute_accuracy(training_data, validation_data, k, p)\n",
    "acc_test = compute_accuracy(training_data, test_data, k, p)\n",
    "print(f\"Validation accuracy: {acc_val:.3f}\")\n",
    "print(f\"Test accuracy: {acc_test:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale the data\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "\n",
    "def scale_data(training_data, validation_data, test_data):\n",
    "    \"\"\"\n",
    "    Scale features using StandardScaler while preserving labels\n",
    "    \"\"\"\n",
    "    # Extract features and labels\n",
    "    train_features = np.array([row[:-1] for row in training_data])\n",
    "    val_features = np.array([row[:-1] for row in validation_data])\n",
    "    test_features = np.array([row[:-1] for row in test_data])\n",
    "    \n",
    "    # Get labels\n",
    "    train_labels = [row[-1] for row in training_data]\n",
    "    val_labels = [row[-1] for row in validation_data]\n",
    "    test_labels = [row[-1] for row in test_data]\n",
    "    \n",
    "    # Scale features\n",
    "    scaler = StandardScaler()\n",
    "    train_scaled = scaler.fit_transform(train_features)\n",
    "    val_scaled = scaler.transform(val_features)\n",
    "    test_scaled = scaler.transform(test_features)\n",
    "    \n",
    "    # Combine scaled features with labels\n",
    "    training_scaled = [list(features) + [label] for features, label in zip(train_scaled, train_labels)]\n",
    "    validation_scaled = [list(features) + [label] for features, label in zip(val_scaled, val_labels)]\n",
    "    test_scaled = [list(features) + [label] for features, label in zip(test_scaled, test_labels)]\n",
    "    \n",
    "    return training_scaled, validation_scaled, test_scaled\n",
    "\n",
    "# Add this after data loading in main():\n",
    "training_data, validation_data, test_data = scale_data(training_data, validation_data, test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute accuracy AFTER scaling\n",
    "acc_val = compute_accuracy(training_data, validation_data, k, p)\n",
    "acc_test = compute_accuracy(training_data, test_data, k, p)\n",
    "print(f\"Validation accuracy AFTER SCALE: {acc_val:.3f}\")\n",
    "print(f\"Test accuracy AFTER SCALE: {acc_test:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2D 2D 2D 2D\n",
    "# Plot decision map 2D\n",
    "if len(attributes) == 2:\n",
    "    plot_decision_map(training_data, attributes, k, p, grid_step)\n",
    "elif len(attributes) == 3:\n",
    "    print(\"3D plot\")\n",
    "    # Plot decision from 3 angles\n",
    "    training_data_01 = [[row[0], row[1], row[-1]] for row in training_data]\n",
    "    training_data_12 = [[row[1], row[2], row[-1]] for row in training_data]\n",
    "    training_data_02 = [[row[0], row[2], row[-1]] for row in training_data]\n",
    "    plot_decision_map(training_data_01, attributes[:2], k, p, grid_step)\n",
    "    plot_decision_map(training_data_12, attributes[1:], k, p, grid_step)\n",
    "    plot_decision_map(training_data_02, [attributes[0], attributes[2]], k, p, grid_step)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_ds_envi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
